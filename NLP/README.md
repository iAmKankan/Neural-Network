## Index
![dark](https://user-images.githubusercontent.com/12748752/141935752-90492d2e-7904-4f9f-a5a1-c4e59ddc3a33.png)
![light](https://user-images.githubusercontent.com/12748752/141935760-406edb8f-cb9b-4e30-9b69-9153b52c28b4.png)

## Sequence-to-sequence models  
![dark](https://user-images.githubusercontent.com/12748752/141935752-90492d2e-7904-4f9f-a5a1-c4e59ddc3a33.png)
* A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an imagesâ€¦etc) and outputs another sequence of items.
* These are deep learning models that have achieved a lot of success in tasks like `machine translation`, `text summarization` and `image captioning`.
* Google Translate started using such a model in production in late 2016. 
> #### In neural machine translation a sequence is a series of words, processed one after another. The output is, likewise, a series of words:
